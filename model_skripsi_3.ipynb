{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocess**"
      ],
      "metadata": {
        "id": "DEc2LliLziW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mount Drive**"
      ],
      "metadata": {
        "id": "M-vhuZ-Szczf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CONNFTXEjkzu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r \"/content/drive/My Drive/Skripsi 2.0/coco_dataset\" /content/"
      ],
      "metadata": {
        "id": "L-R0yiKRUvgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip /content/coco_dataset/images/train2017.zip -d /content/coco_dataset/images"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GaeBZLXnrEoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Library**"
      ],
      "metadata": {
        "id": "ZKQO-ky5zqnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VSuBrr_kFd_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.utils.data as data\n",
        "\n",
        "from torchvision import transforms\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import nltk\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHlBAYcwsjWf",
        "outputId": "13016eec-be73-4edd-d6b4-4a63b40e2400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Class & Function**"
      ],
      "metadata": {
        "id": "LI7utCw2zurK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr0izk42kAFq"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_threshold,\n",
        "        vocab_file=\"./vocab.pkl\",\n",
        "        start_word=\"<start>\",\n",
        "        end_word=\"<end>\",\n",
        "        unk_word=\"<unk>\",\n",
        "        annotations_file=\"/content/coco_dataset/annotations/captions_train2017.json\",\n",
        "        vocab_from_file=False,\n",
        "    ):\n",
        "\n",
        "        self.vocab_threshold = vocab_threshold\n",
        "        self.vocab_file = vocab_file\n",
        "        self.start_word = start_word\n",
        "        self.end_word = end_word\n",
        "        self.unk_word = unk_word\n",
        "        self.annotations_file = annotations_file\n",
        "        self.vocab_from_file = vocab_from_file\n",
        "        self.get_vocab()\n",
        "\n",
        "    def get_vocab(self):\n",
        "        if os.path.exists(self.vocab_file) and self.vocab_from_file:\n",
        "            with open(self.vocab_file, \"rb\") as f:\n",
        "                vocab = pickle.load(f)\n",
        "            self.word2idx = vocab.word2idx\n",
        "            self.idx2word = vocab.idx2word\n",
        "\n",
        "        # create a new vocab file\n",
        "        else:\n",
        "            self.build_vocab()\n",
        "            with open(self.vocab_file, \"wb\") as f:\n",
        "                pickle.dump(self, f)\n",
        "\n",
        "    def build_vocab(self):\n",
        "        self.init_vocab()\n",
        "        self.add_word(self.start_word)\n",
        "        self.add_word(self.end_word)\n",
        "        self.add_word(self.unk_word)\n",
        "        self.add_captions()\n",
        "\n",
        "    def init_vocab(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def add_captions(self):\n",
        "        coco = COCO(self.annotations_file)\n",
        "        counter = Counter()\n",
        "        ids = coco.anns.keys()\n",
        "        for i, idx in enumerate(ids):\n",
        "            caption = str(coco.anns[idx][\"caption\"])\n",
        "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "            counter.update(tokens)\n",
        "\n",
        "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            return self.word2idx[self.unk_word]\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SYSx6i5j7NO"
      },
      "outputs": [],
      "source": [
        "# Coco Dataset\n",
        "\n",
        "class CoCoDataset(data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        transform,\n",
        "        mode,\n",
        "        batch_size,\n",
        "        vocab_threshold,\n",
        "        vocab_file,\n",
        "        start_word,\n",
        "        end_word,\n",
        "        unk_word,\n",
        "        annotations_file,\n",
        "        vocab_from_file,\n",
        "        img_folder,\n",
        "    ):\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.batch_size = batch_size\n",
        "        self.img_folder = img_folder\n",
        "\n",
        "        self.vocab = Vocabulary(\n",
        "            vocab_threshold,\n",
        "            vocab_file,\n",
        "            start_word,\n",
        "            end_word,\n",
        "            unk_word,\n",
        "            annotations_file,\n",
        "            vocab_from_file,\n",
        "        )\n",
        "        if self.mode == \"train\":\n",
        "            self.coco = COCO(annotations_file)\n",
        "            self.ids = list(self.coco.anns.keys())\n",
        "\n",
        "            tokenized_captions = [\n",
        "                nltk.tokenize.word_tokenize(\n",
        "                    str(self.coco.anns[self.ids[index]][\"caption\"]).lower()\n",
        "                )\n",
        "                for index in tqdm(np.arange(len(self.ids)))\n",
        "            ]\n",
        "\n",
        "            self.caption_lengths = [len(token) for token in tokenized_captions]\n",
        "        else:\n",
        "            test_info = json.loads(open(annotations_file).read())\n",
        "            self.paths = [item[\"file_name\"] for item in test_info[\"images\"]]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.mode == \"train\":\n",
        "            ann_id = self.ids[index]\n",
        "            caption = self.coco.anns[ann_id][\"caption\"]\n",
        "            img_id = self.coco.anns[ann_id][\"image_id\"]\n",
        "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
        "\n",
        "            image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
        "            image = self.transform(image)\n",
        "\n",
        "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "            caption = [self.vocab(self.vocab.start_word)]\n",
        "            caption.extend([self.vocab(token) for token in tokens])\n",
        "            caption.append(self.vocab(self.vocab.end_word))\n",
        "            caption = torch.Tensor(caption).long()\n",
        "\n",
        "            return image, caption\n",
        "\n",
        "        else:\n",
        "            path = self.paths[index]\n",
        "\n",
        "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
        "            orig_image = np.array(pil_image)\n",
        "            image = self.transform(pil_image)\n",
        "\n",
        "            return orig_image, image\n",
        "\n",
        "    def get_train_indices(self):\n",
        "        sel_length = np.random.choice(self.caption_lengths)\n",
        "\n",
        "        all_indices = np.where(\n",
        "            [\n",
        "                self.caption_lengths[i] == sel_length\n",
        "                for i in np.arange(len(self.caption_lengths))\n",
        "            ]\n",
        "        )[0]\n",
        "\n",
        "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
        "        return indices\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.ids)\n",
        "        else:\n",
        "            return len(self.paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjdx22OnjxIR"
      },
      "outputs": [],
      "source": [
        "# Get Loader\n",
        "\n",
        "def get_loader(\n",
        "    transform,\n",
        "    mode=\"train\",\n",
        "    batch_size=1,\n",
        "    vocab_threshold=None,\n",
        "    vocab_file=\"./vocab.pkl\",\n",
        "    start_word=\"<start>\",\n",
        "    end_word=\"<end>\",\n",
        "    unk_word=\"<unk>\",\n",
        "    vocab_from_file=True,\n",
        "    num_workers=4,\n",
        "    cocoapi_loc=\"/opt\",\n",
        "):\n",
        "\n",
        "    if mode == \"train\":\n",
        "        img_folder = os.path.join(cocoapi_loc, \"images/train2017/\")\n",
        "        annotations_file = os.path.join(\n",
        "            cocoapi_loc, \"annotations/captions_train2017.json\"\n",
        "        )\n",
        "\n",
        "    elif mode == \"test\":\n",
        "        img_folder = os.path.join(cocoapi_loc, \"images/val2017/\")\n",
        "        annotations_file = os.path.join(\n",
        "            cocoapi_loc, \"annotations/captions_train2017.json\"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "    # COCO caption dataset.\n",
        "    dataset = CoCoDataset(\n",
        "        transform=transform,\n",
        "        mode=mode,\n",
        "        batch_size=batch_size,\n",
        "        vocab_threshold=vocab_threshold,\n",
        "        vocab_file=vocab_file,\n",
        "        start_word=start_word,\n",
        "        end_word=end_word,\n",
        "        unk_word=unk_word,\n",
        "        annotations_file=annotations_file,\n",
        "        vocab_from_file=vocab_from_file,\n",
        "        img_folder=img_folder,\n",
        "    )\n",
        "\n",
        "    if mode == \"train\":\n",
        "        indices = dataset.get_train_indices()\n",
        "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "\n",
        "        data_loader = data.DataLoader(\n",
        "            dataset=dataset,\n",
        "            num_workers=num_workers,\n",
        "            batch_sampler=data.sampler.BatchSampler(\n",
        "                sampler=initial_sampler, batch_size=dataset.batch_size, drop_last=False\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        data_loader = data.DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=dataset.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "        )\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Val Loader\n",
        "\n",
        "def val_get_loader(\n",
        "    transform,\n",
        "    mode=\"valid\",\n",
        "    batch_size=1,\n",
        "    vocab_threshold=None,\n",
        "    vocab_file=\"./vocab.pkl\",\n",
        "    start_word=\"<start>\",\n",
        "    end_word=\"<end>\",\n",
        "    unk_word=\"<unk>\",\n",
        "    vocab_from_file=True,\n",
        "    num_workers=0,\n",
        "    cocoapi_loc=\"/opt\",\n",
        "):\n",
        "\n",
        "    if mode == \"train\":\n",
        "        img_folder = os.path.join(cocoapi_loc, \"images/train2017/\")\n",
        "        annotations_file = os.path.join(\n",
        "            cocoapi_loc, \"annotations/captions_train2017.json\"\n",
        "        )\n",
        "    elif mode == \"test\":\n",
        "        img_folder = os.path.join(cocoapi_loc, \"images/val2017/\")\n",
        "        annotations_file = os.path.join(\n",
        "            cocoapi_loc, \"annotations/captions_val2017.json\"\n",
        "        )\n",
        "    elif mode == \"valid\":\n",
        "        img_folder = os.path.join(cocoapi_loc, \"images/val2017/\")\n",
        "        annotations_file = os.path.join(\n",
        "            cocoapi_loc, \"annotations/captions_val2017.json\"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "    dataset = CoCoDataset(\n",
        "        transform=transform,\n",
        "        mode=mode,\n",
        "        batch_size=batch_size,\n",
        "        vocab_threshold=vocab_threshold,\n",
        "        vocab_file=vocab_file,\n",
        "        start_word=start_word,\n",
        "        end_word=end_word,\n",
        "        unk_word=unk_word,\n",
        "        annotations_file=annotations_file,\n",
        "        vocab_from_file=vocab_from_file,\n",
        "        img_folder=img_folder,\n",
        "    )\n",
        "\n",
        "    if mode == \"train\":\n",
        "        indices = dataset.get_train_indices()\n",
        "\n",
        "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "\n",
        "        data_loader = data.DataLoader(\n",
        "            dataset=dataset,\n",
        "            num_workers=num_workers,\n",
        "            batch_sampler=data.sampler.BatchSampler(\n",
        "                sampler=initial_sampler, batch_size=dataset.batch_size, drop_last=False\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        data_loader = data.DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=dataset.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "        )\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "class CoCoDataset(data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        transform,\n",
        "        mode,\n",
        "        batch_size,\n",
        "        vocab_threshold,\n",
        "        vocab_file,\n",
        "        start_word,\n",
        "        end_word,\n",
        "        unk_word,\n",
        "        annotations_file,\n",
        "        vocab_from_file,\n",
        "        img_folder,\n",
        "    ):\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(\n",
        "            vocab_threshold,\n",
        "            vocab_file,\n",
        "            start_word,\n",
        "            end_word,\n",
        "            unk_word,\n",
        "            annotations_file,\n",
        "            vocab_from_file,\n",
        "        )\n",
        "        self.img_folder = img_folder\n",
        "        if self.mode == \"train\":\n",
        "            self.coco = COCO(annotations_file)\n",
        "            self.ids = list(self.coco.anns.keys())\n",
        "\n",
        "            all_tokens = [\n",
        "                nltk.tokenize.word_tokenize(\n",
        "                    str(self.coco.anns[self.ids[index]][\"caption\"]).lower()\n",
        "                )\n",
        "                for index in tqdm(np.arange(len(self.ids)))\n",
        "            ]\n",
        "            self.caption_lengths = [len(token) for token in all_tokens]\n",
        "        else:\n",
        "            test_info = json.loads(open(annotations_file).read())\n",
        "            self.paths = [item[\"file_name\"] for item in test_info[\"images\"]]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.mode == \"train\":\n",
        "            ann_id = self.ids[index]\n",
        "            caption = self.coco.anns[ann_id][\"caption\"]\n",
        "            img_id = self.coco.anns[ann_id][\"image_id\"]\n",
        "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
        "\n",
        "            image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
        "            image = self.transform(image)\n",
        "\n",
        "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "            caption = []\n",
        "            caption.append(self.vocab(self.vocab.start_word))\n",
        "            caption.extend([self.vocab(token) for token in tokens])\n",
        "            caption.append(self.vocab(self.vocab.end_word))\n",
        "            caption = torch.Tensor(caption).long()\n",
        "\n",
        "            return image, caption\n",
        "\n",
        "        elif self.mode == \"valid\":\n",
        "            path = self.paths[index]\n",
        "            image_id = int(path.split(\"/\")[0].split(\".\")[0].split(\"_\")[-1])\n",
        "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
        "            image = self.transform(pil_image)\n",
        "\n",
        "            return image_id, image\n",
        "\n",
        "        else:\n",
        "            path = self.paths[index]\n",
        "\n",
        "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
        "            orig_image = np.array(pil_image)\n",
        "            image = self.transform(pil_image)\n",
        "\n",
        "            return orig_image, image\n",
        "\n",
        "    def get_train_indices(self):\n",
        "        sel_length = np.random.choice(self.caption_lengths)\n",
        "        all_indices = np.where(\n",
        "            [\n",
        "                self.caption_lengths[i] == sel_length\n",
        "                for i in np.arange(len(self.caption_lengths))\n",
        "            ]\n",
        "        )[0]\n",
        "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
        "        return indices\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.ids)\n",
        "        else:\n",
        "            return len(self.paths)"
      ],
      "metadata": {
        "id": "zpVXi6oWpoZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean sentence\n",
        "\n",
        "def clean_sentence(output, idx2word):\n",
        "    sentence = \"\"\n",
        "    for i in output:\n",
        "        word = idx2word[i]\n",
        "        if i == 0:\n",
        "            continue\n",
        "        if i == 1:\n",
        "            break\n",
        "        if i == 18:\n",
        "            sentence = sentence + word\n",
        "        else:\n",
        "            sentence = sentence + \" \" + word\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "-TICc3Cdsm9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU Score\n",
        "\n",
        "def bleu_score(true_sentences, predicted_sentences):\n",
        "    hypotheses = []\n",
        "    references = []\n",
        "    for img_id in set(true_sentences.keys()).intersection(\n",
        "        set(predicted_sentences.keys())\n",
        "    ):\n",
        "        img_refs = [cap.split() for cap in true_sentences[img_id]]\n",
        "        references.append(img_refs)\n",
        "        hypotheses.append(predicted_sentences[img_id][0].strip().split())\n",
        "\n",
        "    return corpus_bleu(references, hypotheses)"
      ],
      "metadata": {
        "id": "NRZHpt8Hz8se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder & Decoder\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.embed(features)\n",
        "        return features\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_size\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        cap_embedding = self.embed(\n",
        "            captions[:, :-1]\n",
        "        )\n",
        "\n",
        "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
        "\n",
        "        lstm_out, self.hidden = self.lstm(\n",
        "            embeddings\n",
        "        )\n",
        "        outputs = self.linear(lstm_out)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, inputs, states=None, max_len=20):\n",
        "        res = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            lstm_out, states = self.lstm(\n",
        "                inputs, states\n",
        "            )\n",
        "            outputs = self.linear(lstm_out.squeeze(dim=1))\n",
        "            _, predicted_idx = outputs.max(dim=1)\n",
        "            res.append(predicted_idx.item())\n",
        "\n",
        "            if predicted_idx == 1:\n",
        "                break\n",
        "            inputs = self.embed(predicted_idx)\n",
        "\n",
        "            inputs = inputs.unsqueeze(1)\n",
        "\n",
        "        return res"
      ],
      "metadata": {
        "id": "JtDKqMF0ow_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_blwWRxwkyUM"
      },
      "outputs": [],
      "source": [
        "# Pre-process image\n",
        "\n",
        "transform_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.485, 0.456, 0.406),\n",
        "            (0.229, 0.224, 0.225),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cocoapi_dir = '/content/coco_dataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJWTM3LsrbHu"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drt99_cirTuY",
        "outputId": "e54a9696-432a-445f-db48-b4f2e8f190e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Compute unit: GPU\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBq6pdM0Oq1g"
      },
      "outputs": [],
      "source": [
        "# Setting hyperparameters\n",
        "\n",
        "batch_size = 128\n",
        "vocab_threshold = 5\n",
        "vocab_from_file = True\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_epochs = 5\n",
        "save_every = 1\n",
        "print_every = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocab\n",
        "\n",
        "# data_loader = get_loader(\n",
        "#     transform=transform_train,\n",
        "#     mode=\"train\",\n",
        "#     batch_size=batch_size,\n",
        "#     vocab_threshold=vocab_threshold,\n",
        "#     vocab_from_file=False,\n",
        "#     cocoapi_loc=cocoapi_dir,\n",
        "# )"
      ],
      "metadata": {
        "id": "tgWC5asflyFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3hdaj-rPoKM",
        "outputId": "6eede08f-9ba7-4307-f49d-b41ce487d628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary successfully loaded from vocab.pkl file!\n",
            "loading annotations into memory...\n",
            "Done (t=1.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Obtaining caption lengths...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 591753/591753 [00:53<00:00, 11096.37it/s]\n"
          ]
        }
      ],
      "source": [
        "# Set train data loader\n",
        "\n",
        "data_loader = get_loader(\n",
        "    transform=transform_train,\n",
        "    mode=\"train\",\n",
        "    batch_size=batch_size,\n",
        "    vocab_threshold=vocab_threshold,\n",
        "    vocab_from_file=vocab_from_file,\n",
        "    cocoapi_loc=cocoapi_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZal-9WXPuTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14349efe-da35-4667-8316-f8a456eda963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Initialize architecture\n",
        "\n",
        "vocab_size = len(data_loader.dataset.vocab)\n",
        "\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
        "\n",
        "criterion = (\n",
        "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        ")\n",
        "\n",
        "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
        "\n",
        "optimizer = torch.optim.Adam(params, lr=0.001)\n",
        "\n",
        "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define checkpoint path\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/Skripsi 2.0/save_path'\n",
        "os.makedirs(model_save_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "CP7iwk2EsRwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    for i_step in range(1, total_step + 1):\n",
        "        indices = data_loader.dataset.get_train_indices()\n",
        "\n",
        "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "        data_loader.batch_sampler.sampler = new_sampler\n",
        "\n",
        "        images, captions = next(iter(data_loader))\n",
        "\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        stats = (\n",
        "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
        "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
        "        )\n",
        "\n",
        "        if i_step % print_every == 0:\n",
        "            print(\"\\r\" + stats)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % save_every == 0:\n",
        "        torch.save(\n",
        "            decoder.state_dict(), os.path.join(model_save_path, f\"decoder-{epoch}.pkl\")\n",
        "        )\n",
        "        torch.save(\n",
        "            encoder.state_dict(), os.path.join(model_save_path, f\"encoder-{epoch}.pkl\")\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KrzBLA15QUj3",
        "outputId": "56e1cdce-c749-4e7d-b537-85994d1e06c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/2312], Loss: 3.8825, Perplexity: 48.5457\n",
            "Epoch [1/5], Step [200/2312], Loss: 3.1019, Perplexity: 22.2408\n",
            "Epoch [1/5], Step [300/2312], Loss: 2.8584, Perplexity: 17.4331\n",
            "Epoch [1/5], Step [400/2312], Loss: 3.0613, Perplexity: 21.3549\n",
            "Epoch [1/5], Step [500/2312], Loss: 2.5684, Perplexity: 13.0455\n",
            "Epoch [1/5], Step [600/2312], Loss: 2.8741, Perplexity: 17.7097\n",
            "Epoch [1/5], Step [700/2312], Loss: 2.7965, Perplexity: 16.3874\n",
            "Epoch [1/5], Step [800/2312], Loss: 2.4042, Perplexity: 11.0701\n",
            "Epoch [1/5], Step [900/2312], Loss: 2.3060, Perplexity: 10.0342\n",
            "Epoch [1/5], Step [1000/2312], Loss: 2.5846, Perplexity: 13.2584\n",
            "Epoch [1/5], Step [1100/2312], Loss: 2.2560, Perplexity: 9.5449\n",
            "Epoch [1/5], Step [1200/2312], Loss: 2.2348, Perplexity: 9.3446\n",
            "Epoch [1/5], Step [1300/2312], Loss: 2.3044, Perplexity: 10.0182\n",
            "Epoch [1/5], Step [1400/2312], Loss: 2.3209, Perplexity: 10.1847\n",
            "Epoch [1/5], Step [1500/2312], Loss: 2.3148, Perplexity: 10.1230\n",
            "Epoch [1/5], Step [1600/2312], Loss: 2.2181, Perplexity: 9.1895\n",
            "Epoch [1/5], Step [1700/2312], Loss: 2.1235, Perplexity: 8.3605\n",
            "Epoch [1/5], Step [1800/2312], Loss: 2.2220, Perplexity: 9.2253\n",
            "Epoch [1/5], Step [1900/2312], Loss: 2.0680, Perplexity: 7.9089\n",
            "Epoch [1/5], Step [2000/2312], Loss: 2.5825, Perplexity: 13.2305\n",
            "Epoch [1/5], Step [2100/2312], Loss: 2.3329, Perplexity: 10.3080\n",
            "Epoch [1/5], Step [2200/2312], Loss: 2.0354, Perplexity: 7.6552\n",
            "Epoch [1/5], Step [2300/2312], Loss: 2.1121, Perplexity: 8.2653\n",
            "Epoch [2/5], Step [100/2312], Loss: 1.9906, Perplexity: 7.3201\n",
            "Epoch [2/5], Step [200/2312], Loss: 2.0070, Perplexity: 7.4413\n",
            "Epoch [2/5], Step [300/2312], Loss: 2.0624, Perplexity: 7.8644\n",
            "Epoch [2/5], Step [400/2312], Loss: 2.2821, Perplexity: 9.7974\n",
            "Epoch [2/5], Step [500/2312], Loss: 1.9935, Perplexity: 7.3411\n",
            "Epoch [2/5], Step [600/2312], Loss: 2.1250, Perplexity: 8.3729\n",
            "Epoch [2/5], Step [700/2312], Loss: 2.0043, Perplexity: 7.4209\n",
            "Epoch [2/5], Step [800/2312], Loss: 2.0562, Perplexity: 7.8166\n",
            "Epoch [2/5], Step [900/2312], Loss: 2.0714, Perplexity: 7.9358\n",
            "Epoch [2/5], Step [1000/2312], Loss: 2.0094, Perplexity: 7.4585\n",
            "Epoch [2/5], Step [1100/2312], Loss: 1.9808, Perplexity: 7.2482\n",
            "Epoch [2/5], Step [1200/2312], Loss: 2.0042, Perplexity: 7.4199\n",
            "Epoch [2/5], Step [1300/2312], Loss: 2.0307, Perplexity: 7.6195\n",
            "Epoch [2/5], Step [1400/2312], Loss: 1.9749, Perplexity: 7.2062\n",
            "Epoch [2/5], Step [1500/2312], Loss: 1.8441, Perplexity: 6.3221\n",
            "Epoch [2/5], Step [1600/2312], Loss: 2.0200, Perplexity: 7.5382\n",
            "Epoch [2/5], Step [1700/2312], Loss: 2.0189, Perplexity: 7.5299\n",
            "Epoch [2/5], Step [1800/2312], Loss: 2.2314, Perplexity: 9.3132\n",
            "Epoch [2/5], Step [1900/2312], Loss: 1.9526, Perplexity: 7.0472\n",
            "Epoch [2/5], Step [2000/2312], Loss: 1.9476, Perplexity: 7.0116\n",
            "Epoch [2/5], Step [2100/2312], Loss: 1.9300, Perplexity: 6.8899\n",
            "Epoch [2/5], Step [2200/2312], Loss: 1.9710, Perplexity: 7.1779\n",
            "Epoch [2/5], Step [2300/2312], Loss: 2.0464, Perplexity: 7.7400\n",
            "Epoch [3/5], Step [100/2312], Loss: 2.2507, Perplexity: 9.4939\n",
            "Epoch [3/5], Step [200/2312], Loss: 1.8259, Perplexity: 6.2083\n",
            "Epoch [3/5], Step [300/2312], Loss: 1.8784, Perplexity: 6.5432\n",
            "Epoch [3/5], Step [400/2312], Loss: 2.0786, Perplexity: 7.9931\n",
            "Epoch [3/5], Step [500/2312], Loss: 1.8516, Perplexity: 6.3703\n",
            "Epoch [3/5], Step [600/2312], Loss: 1.8471, Perplexity: 6.3411\n",
            "Epoch [3/5], Step [700/2312], Loss: 1.8098, Perplexity: 6.1090\n",
            "Epoch [3/5], Step [800/2312], Loss: 2.4594, Perplexity: 11.6980\n",
            "Epoch [3/5], Step [900/2312], Loss: 2.7000, Perplexity: 14.8804\n",
            "Epoch [3/5], Step [1000/2312], Loss: 2.4689, Perplexity: 11.8093\n",
            "Epoch [3/5], Step [1100/2312], Loss: 2.0001, Perplexity: 7.3900\n",
            "Epoch [3/5], Step [1200/2312], Loss: 1.8031, Perplexity: 6.0687\n",
            "Epoch [3/5], Step [1300/2312], Loss: 1.9807, Perplexity: 7.2477\n",
            "Epoch [3/5], Step [1400/2312], Loss: 1.8884, Perplexity: 6.6090\n",
            "Epoch [3/5], Step [1500/2312], Loss: 1.8635, Perplexity: 6.4460\n",
            "Epoch [3/5], Step [1600/2312], Loss: 2.2052, Perplexity: 9.0717\n",
            "Epoch [3/5], Step [1700/2312], Loss: 1.8710, Perplexity: 6.4950\n",
            "Epoch [3/5], Step [1800/2312], Loss: 1.8447, Perplexity: 6.3263\n",
            "Epoch [3/5], Step [1900/2312], Loss: 1.8406, Perplexity: 6.3001\n",
            "Epoch [3/5], Step [2000/2312], Loss: 1.8187, Perplexity: 6.1636\n",
            "Epoch [3/5], Step [2100/2312], Loss: 1.8814, Perplexity: 6.5628\n",
            "Epoch [3/5], Step [2200/2312], Loss: 1.8303, Perplexity: 6.2358\n",
            "Epoch [3/5], Step [2300/2312], Loss: 1.9707, Perplexity: 7.1755\n",
            "Epoch [4/5], Step [100/2312], Loss: 1.8612, Perplexity: 6.4314\n",
            "Epoch [4/5], Step [200/2312], Loss: 1.8951, Perplexity: 6.6534\n",
            "Epoch [4/5], Step [300/2312], Loss: 2.0537, Perplexity: 7.7969\n",
            "Epoch [4/5], Step [400/2312], Loss: 1.8387, Perplexity: 6.2881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final epoch\n",
        "\n",
        "torch.save(decoder.state_dict(), os.path.join(model_save_path, 'decoder-final.pkl'))\n",
        "torch.save(encoder.state_dict(), os.path.join(model_save_path, 'encoder-final.pkl'))"
      ],
      "metadata": {
        "id": "B6MGF7ivpBio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation step\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.485, 0.456, 0.406),\n",
        "            (0.229, 0.224, 0.225),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Create the data loader\n",
        "val_data_loader = val_get_loader(\n",
        "    transform=transform_test, mode=\"valid\", cocoapi_loc=cocoapi_dir\n",
        ")\n",
        "\n",
        "encoder_file = \"encoder-3.pt\"\n",
        "decoder_file = \"decoder-3.pt\"\n",
        "\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
        "\n",
        "# Load the trained weights\n",
        "encoder.load_state_dict(torch.load(os.path.join(model_save_path, encoder_file)))\n",
        "decoder.load_state_dict(torch.load(os.path.join(model_save_path, decoder_file)))\n",
        "\n",
        "# Set to eval\n",
        "encoder.eval()\n",
        "decoder.eval()"
      ],
      "metadata": {
        "id": "D8KdptxspSpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a771ec2-f076-4b11-e219-c4c1088752e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary successfully loaded from vocab.pkl file!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-cf715f050412>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  encoder.load_state_dict(torch.load(os.path.join(model_save_path, encoder_file)))\n",
            "<ipython-input-21-cf715f050412>:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder.load_state_dict(torch.load(os.path.join(model_save_path, decoder_file)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderRNN(\n",
              "  (embed): Embedding(2334, 256)\n",
              "  (lstm): LSTM(256, 512, batch_first=True)\n",
              "  (linear): Linear(in_features=512, out_features=2334, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Infer captions for all images\n",
        "\n",
        "pred_result = defaultdict(list)\n",
        "for img_id, img in tqdm(val_data_loader):\n",
        "    img = img.to(device)\n",
        "    with torch.no_grad():\n",
        "        features = encoder(img)\n",
        "        features = features.unsqueeze(0).permute(1, 0, 2)\n",
        "        output = decoder.sample(features)\n",
        "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
        "    pred_result[img_id.item()].append(sentence)"
      ],
      "metadata": {
        "id": "X1qgNZK9h3xC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b4bb721-5134-42b1-adb0-8795e9f53bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [01:41<00:00, 49.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAPYdSBBn5Ru",
        "outputId": "b8e90d59-1afe-4a1a-92eb-a841b9b416b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 3, 74, 60, 52, 53, 3, 240, 11, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\n",
        "    os.path.join(cocoapi_dir, \"annotations/captions_val2017.json\"), \"r\"\n",
        ") as f:\n",
        "    caption = json.load(f)\n",
        "\n",
        "valid_annot = caption[\"annotations\"]\n",
        "valid_result = defaultdict(list)\n",
        "for i in valid_annot:\n",
        "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
      ],
      "metadata": {
        "id": "29i5I4GyrLdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(valid_result.values())[:3]"
      ],
      "metadata": {
        "id": "9XHFwX_q0eCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb3bf4b-c84d-4a40-93ae-16e455e1babc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['a black honda motorcycle parked in front of a garage.',\n",
              "  'a honda motorcycle parked in a grass driveway',\n",
              "  'a black honda motorcycle with a dark burgundy seat.',\n",
              "  'ma motorcycle parked on the gravel in front of a garage',\n",
              "  'a motorcycle with its brake extended standing outside'],\n",
              " ['an office cubicle with four different types of computers.',\n",
              "  'the home office space seems to be very cluttered.',\n",
              "  'an office with desk computer and chair and laptop.',\n",
              "  'office setting with a lot of computer screens.',\n",
              "  'a desk and chair in an office cubicle.'],\n",
              " ['a small closed toilet in a cramped space.',\n",
              "  'a tan toilet and sink combination in a small room.',\n",
              "  'this is an advanced toilet with a sink and control panel.',\n",
              "  'a close-up picture of a toilet with a fountain.',\n",
              "  'off white toilet with a faucet and controls. ']]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(pred_result.values())[:3]"
      ],
      "metadata": {
        "id": "M28vyu9b0i4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13876d93-5303-4208-8ce0-eba1ca9e2321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[' a man is sitting on a table .'],\n",
              " [' a man is sitting on a table .'],\n",
              " [' a man is sitting on a table .']]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
      ],
      "metadata": {
        "id": "yjl2AK_y0l-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fd4818-bfe1-437a-d218-57b6a738f9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.056271270164749254"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Test**"
      ],
      "metadata": {
        "id": "Fp1sdHCxvE1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the test data loader\n",
        "\n",
        "data_loader = get_loader(transform=transform_test, mode=\"test\", cocoapi_loc=cocoapi_dir)"
      ],
      "metadata": {
        "id": "AXf44L32jaBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict caption\n",
        "\n",
        "def get_prediction(idx2word, i=0, save=False):\n",
        "    orig_image, image = next(iter(data_loader))\n",
        "    image = image.to(device)\n",
        "    features = encoder(image).unsqueeze(1)\n",
        "    output = decoder.sample(features)\n",
        "    sentence = clean_sentence(output, idx2word)\n",
        "\n",
        "    ax = plt.axes()\n",
        "\n",
        "    ax.spines[\"right\"].set_visible(False)\n",
        "    ax.spines[\"top\"].set_visible(False)\n",
        "    ax.spines[\"bottom\"].set_visible(False)\n",
        "    ax.spines[\"left\"].set_visible(False)\n",
        "\n",
        "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
        "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
        "\n",
        "    plt.imshow(np.squeeze(orig_image))\n",
        "    plt.xlabel(sentence, fontsize=12)\n",
        "    if save:\n",
        "        plt.savefig(f\"samples/sample_{i:03}.png\", bbox_inches=\"tight\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nbDNhmOsj6oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    get_prediction(data_loader.dataset.vocab.idx2word, i=i)"
      ],
      "metadata": {
        "id": "JZMq60vClh_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}